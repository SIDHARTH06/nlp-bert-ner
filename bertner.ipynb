{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2023-11-10T15:48:22.792334Z","iopub.execute_input":"2023-11-10T15:48:22.792708Z","iopub.status.idle":"2023-11-10T15:48:22.804099Z","shell.execute_reply.started":"2023-11-10T15:48:22.792672Z","shell.execute_reply":"2023-11-10T15:48:22.802842Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom transformers import AutoTokenizer,BertTokenizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.optim import Adam\nimport torch.nn as nn\nimport torch\nfrom transformers import BertTokenizer,BertModel, BertConfig\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-10T15:48:22.806186Z","iopub.execute_input":"2023-11-10T15:48:22.806636Z","iopub.status.idle":"2023-11-10T15:48:35.643562Z","shell.execute_reply.started":"2023-11-10T15:48:22.806600Z","shell.execute_reply":"2023-11-10T15:48:35.642684Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2023-11-10T15:48:35.644565Z","iopub.execute_input":"2023-11-10T15:48:35.644855Z","iopub.status.idle":"2023-11-10T15:48:35.673736Z","shell.execute_reply.started":"2023-11-10T15:48:35.644829Z","shell.execute_reply":"2023-11-10T15:48:35.672555Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"TRAIN_PATH='/kaggle/input/conll003-englishversion/train.txt'\nTEST_PATH='/kaggle/input/conll003-englishversion/test.txt'\nVALID_PATH='/kaggle/input/conll003-englishversion/valid.txt'\nLABEL_ARRAY=['O','B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-MISC', 'I-MISC']\nBATCH_SIZE=16\nNUM_EPOCHS=50\nMAX_LENGTH=512\nLEARNING_RATE=0.0001","metadata":{"execution":{"iopub.status.busy":"2023-11-10T15:48:35.676226Z","iopub.execute_input":"2023-11-10T15:48:35.676601Z","iopub.status.idle":"2023-11-10T15:48:35.683527Z","shell.execute_reply.started":"2023-11-10T15:48:35.676566Z","shell.execute_reply":"2023-11-10T15:48:35.682558Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def preprocess(data):\n    sentences=[]\n    labels=[]\n    current_sentence = []\n    current_labels = []\n    for line in data:\n        if line == '\\n':\n            sentences.append(' '.join(current_sentence))\n            labels.append(current_labels)\n            current_sentence = []\n            current_labels = []\n        else:\n            parts = line.strip().split()\n            current_sentence.append(parts[0])\n            current_labels.append(parts[-1])\n    sentences=sentences[1:]\n    labels=labels[1:]\n    return sentences,labels","metadata":{"execution":{"iopub.status.busy":"2023-11-10T15:48:35.684793Z","iopub.execute_input":"2023-11-10T15:48:35.685097Z","iopub.status.idle":"2023-11-10T15:48:35.693602Z","shell.execute_reply.started":"2023-11-10T15:48:35.685071Z","shell.execute_reply":"2023-11-10T15:48:35.692652Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class NERDataset(Dataset):\n    def __init__(self,data_path=TRAIN_PATH):\n        super(NERDataset, self).__init__()\n        with open(data_path,'r') as f:\n            data=f.readlines()\n            self.data=data[1:]\n        self.labencoder=LabelEncoder()\n        labelarray = LABEL_ARRAY\n        self.labencoder.fit(labelarray)\n        self.model_name = \"bert-base-cased\"\n        self.tokenizer = BertTokenizer.from_pretrained(self.model_name)\n        self.sentences,self.labels=preprocess(self.data)\n    def __len__(self):\n        return len(self.sentences)\n    def __getitem__(self,idx):\n        sen=self.sentences[idx]\n        lab=self.labels[idx]\n        lab=['O' if i == 'B-LOC' or i == 'I-LOC' else i for i in lab]\n        lab=self.labencoder.transform(lab)\n        l=[*lab, *[0]*(MAX_LENGTH-len(lab))]\n        self.features=self.tokenizer.encode_plus(sen,add_special_tokens=True,max_length=MAX_LENGTH,pad_to_max_length=True,return_attention_mask=True,return_token_type_ids=False,return_tensors='pt')\n        return torch.tensor(self.features['input_ids'].squeeze()),torch.tensor(self.features['attention_mask'].squeeze()),torch.tensor(l)","metadata":{"execution":{"iopub.status.busy":"2023-11-10T15:48:35.694787Z","iopub.execute_input":"2023-11-10T15:48:35.695089Z","iopub.status.idle":"2023-11-10T15:48:35.705442Z","shell.execute_reply.started":"2023-11-10T15:48:35.695064Z","shell.execute_reply":"2023-11-10T15:48:35.704493Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_dataset=NERDataset()\ntest_dataset=NERDataset(data_path=TEST_PATH)\nvalid_dataset=NERDataset(data_path=VALID_PATH)","metadata":{"execution":{"iopub.status.busy":"2023-11-10T15:48:35.706584Z","iopub.execute_input":"2023-11-10T15:48:35.706921Z","iopub.status.idle":"2023-11-10T15:48:38.885228Z","shell.execute_reply.started":"2023-11-10T15:48:35.706895Z","shell.execute_reply":"2023-11-10T15:48:38.884339Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"408edab820874de986f9766c8de23a08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3669a8527fac4f0d808b3dd837f26938"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b72a792431b4259afb36e79b8dedfbf"}},"metadata":{}}]},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-10T15:48:38.886395Z","iopub.execute_input":"2023-11-10T15:48:38.886724Z","iopub.status.idle":"2023-11-10T15:48:38.894055Z","shell.execute_reply.started":"2023-11-10T15:48:38.886697Z","shell.execute_reply":"2023-11-10T15:48:38.893092Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class NERModel(nn.Module):\n    def __init__(self, num_labels=len(LABEL_ARRAY)):\n        super(NERModel, self).__init__()\n        self.bert = BertModel.from_pretrained(\"bert-base-cased\")\n        self.dropout = nn.Dropout(0.1)\n        self.linear1 = nn.Linear(self.bert.config.hidden_size, 256)\n        self.relu=nn.ReLU()\n        self.linear2 = nn.Linear(256, 32)\n        self.softmax = nn.Softmax(num_labels)\n    def forward(self, i,a):\n        outputs= self.bert(i,a)\n        sequence_output = outputs.last_hidden_state\n#         print(sequence_output.shape)\n        logits = self.linear1(sequence_output)\n        logits = self.relu(logits)\n        logits=self.linear2(logits)\n        print(logits.shape)\n        logits = self.softmax(logits)\n        print(logits.shape)\n        return logits","metadata":{"execution":{"iopub.status.busy":"2023-11-10T15:48:38.895207Z","iopub.execute_input":"2023-11-10T15:48:38.895594Z","iopub.status.idle":"2023-11-10T15:48:38.904761Z","shell.execute_reply.started":"2023-11-10T15:48:38.895560Z","shell.execute_reply":"2023-11-10T15:48:38.903751Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"model = NERModel(num_labels=len(LABEL_ARRAY))\nmodel.to(device)\noptimizer = Adam(model.parameters(), lr=LEARNING_RATE)","metadata":{"execution":{"iopub.status.busy":"2023-11-10T15:48:38.907621Z","iopub.execute_input":"2023-11-10T15:48:38.908058Z","iopub.status.idle":"2023-11-10T15:48:57.967305Z","shell.execute_reply.started":"2023-11-10T15:48:38.908022Z","shell.execute_reply":"2023-11-10T15:48:57.966441Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6726b491116741f08b437559ce1ec0c7"}},"metadata":{}}]},{"cell_type":"code","source":"lossfn=nn.CrossEntropyLoss()\ntotal_training_loss=0.0","metadata":{"execution":{"iopub.status.busy":"2023-11-10T15:48:57.968481Z","iopub.execute_input":"2023-11-10T15:48:57.968867Z","iopub.status.idle":"2023-11-10T15:48:57.976054Z","shell.execute_reply.started":"2023-11-10T15:48:57.968830Z","shell.execute_reply":"2023-11-10T15:48:57.974586Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"for i in train_loader:\n    ins,a,l=i\n    print(ins,ins.shape)\n    print(a,a.shape)\n    print(l,l.shape)\n    break","metadata":{"execution":{"iopub.status.busy":"2023-11-10T15:51:13.098950Z","iopub.execute_input":"2023-11-10T15:51:13.099920Z","iopub.status.idle":"2023-11-10T15:51:13.137029Z","shell.execute_reply.started":"2023-11-10T15:51:13.099878Z","shell.execute_reply":"2023-11-10T15:51:13.135991Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"tensor([[  101,  1275,   119,  ...,     0,     0,     0],\n        [  101, 14444, 18445,  ...,     0,     0,     0],\n        [  101,   157, 10781,  ...,     0,     0,     0],\n        ...,\n        [  101,   107,   146,  ...,     0,     0,     0],\n        [  101,  1252,  1175,  ...,     0,     0,     0],\n        [  101,  1124,  1125,  ...,     0,     0,     0]]) torch.Size([16, 512])\ntensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        ...,\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0]]) torch.Size([16, 512])\ntensor([[6, 2, 5,  ..., 0, 0, 0],\n        [1, 4, 6,  ..., 0, 0, 0],\n        [1, 4, 6,  ..., 0, 0, 0],\n        ...,\n        [6, 6, 6,  ..., 0, 0, 0],\n        [6, 6, 6,  ..., 0, 0, 0],\n        [6, 6, 6,  ..., 0, 0, 0]]) torch.Size([16, 512])\n","output_type":"stream"}]},{"cell_type":"code","source":"training_loss=[]\nmodel.train()\nprint(\"Training Model\")\nfor epoch in tqdm(range(NUM_EPOCHS)):\n    for i,batch in enumerate(train_loader):\n#         print(f\"batch {i} / {len(train_loader)} training.....\")\n        inp,attn,l= batch\n        optimizer.zero_grad()\n        inp=torch.tensor(inp,device=device)\n        attn=torch.tensor(attn,device=device)\n        l=torch.tensor(l,device=device)\n        logits=model(inp,attn)\n        loss=lossfn(logits,l[:,:len(LABEL_ARRAY)])\n        loss.backward()\n        optimizer.step()\n        total_training_loss+=loss.item()\n    avg_loss = total_training_loss / len(train_loader)\n    training_loss.append(avg_loss)\n    print(f\"Epoch {epoch + 1} - Average Loss: {avg_loss:.4f}\")\nprint(\"Training Done\")\ntorch.save(model,'model.pth')\nprint(\"Testing Model\")","metadata":{"execution":{"iopub.status.busy":"2023-11-10T15:48:57.977362Z","iopub.execute_input":"2023-11-10T15:48:57.977723Z","iopub.status.idle":"2023-11-10T15:49:02.317709Z","shell.execute_reply.started":"2023-11-10T15:48:57.977691Z","shell.execute_reply":"2023-11-10T15:49:02.316391Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Training Model\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/50 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n  0%|          | 0/50 [00:01<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"torch.Size([16, 512, 32])\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m attn\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(attn,device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     11\u001b[0m l\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(l,device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 12\u001b[0m logits\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m loss\u001b[38;5;241m=\u001b[39mlossfn(logits,l[:,:\u001b[38;5;28mlen\u001b[39m(LABEL_ARRAY)])\n\u001b[1;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[9], line 18\u001b[0m, in \u001b[0;36mNERModel.forward\u001b[0;34m(self, i, a)\u001b[0m\n\u001b[1;32m     16\u001b[0m logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(logits)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(logits\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 18\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(logits\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/activation.py:1459\u001b[0m, in \u001b[0;36mSoftmax.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1843\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1841\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   1842\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1843\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1844\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1845\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n","\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-3, 2], but got 7)"],"ename":"IndexError","evalue":"Dimension out of range (expected to be in range of [-3, 2], but got 7)","output_type":"error"}]},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    total_correct = 0\n    total_samples = 0\n    for i,batch in enumerate(val_loader):\n        inp,attn,l= batch\n        inp=torch.tensor(inp,device=device)\n        attn=torch.tensor(attn,device=device)\n        l=torch.tensor(l,device=device)\n        logits = model(inp, attn)\n        print(inp.shape)\n        total_correct += (logits == l).sum().item()\n        total_samples += l.size(0) * l.size(1)\n\n    accuracy = total_correct / total_samples\n    print(f\"Validation Accuracy: {accuracy}\")\nprint(\"Testing Done\")","metadata":{"execution":{"iopub.status.busy":"2023-11-10T15:49:02.318408Z","iopub.status.idle":"2023-11-10T15:49:02.318759Z","shell.execute_reply.started":"2023-11-10T15:49:02.318594Z","shell.execute_reply":"2023-11-10T15:49:02.318611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(np.array(training_loss))\nplt.xlabel('Epoch')\nplt.ylabel('Training Loss')\nplt.title('Training History')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-10T15:49:02.319782Z","iopub.status.idle":"2023-11-10T15:49:02.320096Z","shell.execute_reply.started":"2023-11-10T15:49:02.319935Z","shell.execute_reply":"2023-11-10T15:49:02.319949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}